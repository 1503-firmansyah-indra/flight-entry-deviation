{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5b4973a-9256-4d29-ac2f-dcc68e7a10b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from shapely.geometry import Point, Polygon, LineString, MultiLineString\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import folium\n",
    "\n",
    "from geopy import distance as geo_distance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e019b776-525d-4ed6-87d6-490207634c91",
   "metadata": {},
   "source": [
    "# Load Sectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10d8f665-fe9d-4012-b098-4de0f3f9da0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_sector_from_folder(folder: str):\n",
    "    subsector_list = []\n",
    "    for path in os.listdir(folder):\n",
    "        if not path.endswith('.txt'):\n",
    "            continue\n",
    "        with open(folder+'/'+path) as f:\n",
    "            sec_txt = f.read()\n",
    "        str_cords = sec_txt.split(\" - \")\n",
    "        cords = []\n",
    "        for c in str_cords:\n",
    "            lat = int(c[0:2]) + int(c[2:4])/60 + int(c[4:6])/3600\n",
    "            lon = int(c[8:11]) + int(c[11:13])/60 + int(c[13:15])/3600\n",
    "            cords.append((lon,lat))\n",
    "        subsector_list.append(\n",
    "            gpd.GeoDataFrame(\n",
    "                index=[0],\n",
    "                crs='epsg:4326',\n",
    "                geometry=[Polygon(cords)]\n",
    "            )\n",
    "        )\n",
    "    assert len(subsector_list) > 0\n",
    "    if len(subsector_list) == 1:\n",
    "        return subsector_list[0]\n",
    "    else:\n",
    "        return gpd.GeoDataFrame(\n",
    "            pd.concat(subsector_list, ignore_index=True),\n",
    "            crs=subsector_list[0].crs\n",
    "        ).dissolve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba9ee445-0d7a-4393-84bb-f5d4a08f8577",
   "metadata": {},
   "outputs": [],
   "source": [
    "sector_w_esmm = read_sector_from_folder('./raw_data/sectors_info/esmm_acc_sector_W')\n",
    "sector_y_esmm = read_sector_from_folder('./raw_data/sectors_info/esmm_acc_sector_Y')\n",
    "sector_6_esmm = read_sector_from_folder('./raw_data/sectors_info/esmm_acc_sector_6')\n",
    "sector_67Y_list = [sector_y_esmm, sector_6_esmm]\n",
    "sector_67Y = gpd.GeoDataFrame(\n",
    "    pd.concat(sector_67Y_list, ignore_index=True),\n",
    "    crs='epsg:4326'\n",
    ").dissolve()\n",
    "sectors_list = [\n",
    "    {\n",
    "        \"name\": \"sector_67Y\",\n",
    "        \"sector_gpd\": sector_67Y\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"sector_w_esmm\",\n",
    "        \"sector_gpd\": sector_w_esmm\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c99519a-1951-44b7-8c3f-057b2dc04df4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"width:100%;\"><div style=\"position:relative;width:100%;height:0;padding-bottom:60%;\"><span style=\"color:#565656\">Make this Notebook Trusted to load map: File -> Trust Notebook</span><iframe srcdoc=\"&lt;!DOCTYPE html&gt;\n",
       "&lt;head&gt;    \n",
       "    &lt;meta http-equiv=&quot;content-type&quot; content=&quot;text/html; charset=UTF-8&quot; /&gt;\n",
       "    \n",
       "        &lt;script&gt;\n",
       "            L_NO_TOUCH = false;\n",
       "            L_DISABLE_3D = false;\n",
       "        &lt;/script&gt;\n",
       "    \n",
       "    &lt;style&gt;html, body {width: 100%;height: 100%;margin: 0;padding: 0;}&lt;/style&gt;\n",
       "    &lt;style&gt;#map {position:absolute;top:0;bottom:0;right:0;left:0;}&lt;/style&gt;\n",
       "    &lt;script src=&quot;https://cdn.jsdelivr.net/npm/leaflet@1.6.0/dist/leaflet.js&quot;&gt;&lt;/script&gt;\n",
       "    &lt;script src=&quot;https://code.jquery.com/jquery-1.12.4.min.js&quot;&gt;&lt;/script&gt;\n",
       "    &lt;script src=&quot;https://maxcdn.bootstrapcdn.com/bootstrap/3.2.0/js/bootstrap.min.js&quot;&gt;&lt;/script&gt;\n",
       "    &lt;script src=&quot;https://cdnjs.cloudflare.com/ajax/libs/Leaflet.awesome-markers/2.0.2/leaflet.awesome-markers.js&quot;&gt;&lt;/script&gt;\n",
       "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdn.jsdelivr.net/npm/leaflet@1.6.0/dist/leaflet.css&quot;/&gt;\n",
       "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://maxcdn.bootstrapcdn.com/bootstrap/3.2.0/css/bootstrap.min.css&quot;/&gt;\n",
       "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://maxcdn.bootstrapcdn.com/bootstrap/3.2.0/css/bootstrap-theme.min.css&quot;/&gt;\n",
       "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://maxcdn.bootstrapcdn.com/font-awesome/4.6.3/css/font-awesome.min.css&quot;/&gt;\n",
       "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdnjs.cloudflare.com/ajax/libs/Leaflet.awesome-markers/2.0.2/leaflet.awesome-markers.css&quot;/&gt;\n",
       "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdn.jsdelivr.net/gh/python-visualization/folium/folium/templates/leaflet.awesome.rotate.min.css&quot;/&gt;\n",
       "    \n",
       "            &lt;meta name=&quot;viewport&quot; content=&quot;width=device-width,\n",
       "                initial-scale=1.0, maximum-scale=1.0, user-scalable=no&quot; /&gt;\n",
       "            &lt;style&gt;\n",
       "                #map_ae6534ca6824cc061b07c61e04e13c0b {\n",
       "                    position: relative;\n",
       "                    width: 100.0%;\n",
       "                    height: 100.0%;\n",
       "                    left: 0.0%;\n",
       "                    top: 0.0%;\n",
       "                }\n",
       "            &lt;/style&gt;\n",
       "        \n",
       "&lt;/head&gt;\n",
       "&lt;body&gt;    \n",
       "    \n",
       "            &lt;div class=&quot;folium-map&quot; id=&quot;map_ae6534ca6824cc061b07c61e04e13c0b&quot; &gt;&lt;/div&gt;\n",
       "        \n",
       "&lt;/body&gt;\n",
       "&lt;script&gt;    \n",
       "    \n",
       "            var map_ae6534ca6824cc061b07c61e04e13c0b = L.map(\n",
       "                &quot;map_ae6534ca6824cc061b07c61e04e13c0b&quot;,\n",
       "                {\n",
       "                    center: [58.032426126871336, 19.068391497182084],\n",
       "                    crs: L.CRS.EPSG3857,\n",
       "                    zoom: 6,\n",
       "                    zoomControl: true,\n",
       "                    preferCanvas: false,\n",
       "                }\n",
       "            );\n",
       "\n",
       "            \n",
       "\n",
       "        \n",
       "    \n",
       "            var tile_layer_c9f01e771fe76be70baf7e6863b47b16 = L.tileLayer(\n",
       "                &quot;https://cartodb-basemaps-{s}.global.ssl.fastly.net/light_all/{z}/{x}/{y}.png&quot;,\n",
       "                {&quot;attribution&quot;: &quot;\\u0026copy; \\u003ca href=\\&quot;http://www.openstreetmap.org/copyright\\&quot;\\u003eOpenStreetMap\\u003c/a\\u003e contributors \\u0026copy; \\u003ca href=\\&quot;http://cartodb.com/attributions\\&quot;\\u003eCartoDB\\u003c/a\\u003e, CartoDB \\u003ca href =\\&quot;http://cartodb.com/attributions\\&quot;\\u003eattributions\\u003c/a\\u003e&quot;, &quot;detectRetina&quot;: false, &quot;maxNativeZoom&quot;: 18, &quot;maxZoom&quot;: 18, &quot;minZoom&quot;: 0, &quot;noWrap&quot;: false, &quot;opacity&quot;: 1, &quot;subdomains&quot;: &quot;abc&quot;, &quot;tms&quot;: false}\n",
       "            ).addTo(map_ae6534ca6824cc061b07c61e04e13c0b);\n",
       "        \n",
       "    \n",
       "        function geo_json_4d5e4cf27d48cce90deb9f97bac20397_styler(feature) {\n",
       "            switch(feature.id) {\n",
       "                default:\n",
       "                    return {&quot;color&quot;: &quot;blue&quot;};\n",
       "            }\n",
       "        }\n",
       "\n",
       "        function geo_json_4d5e4cf27d48cce90deb9f97bac20397_onEachFeature(feature, layer) {\n",
       "            layer.on({\n",
       "            });\n",
       "        };\n",
       "        var geo_json_4d5e4cf27d48cce90deb9f97bac20397 = L.geoJson(null, {\n",
       "                onEachFeature: geo_json_4d5e4cf27d48cce90deb9f97bac20397_onEachFeature,\n",
       "            \n",
       "                style: geo_json_4d5e4cf27d48cce90deb9f97bac20397_styler,\n",
       "        });\n",
       "\n",
       "        function geo_json_4d5e4cf27d48cce90deb9f97bac20397_add (data) {\n",
       "            geo_json_4d5e4cf27d48cce90deb9f97bac20397\n",
       "                .addData(data)\n",
       "                .addTo(map_ae6534ca6824cc061b07c61e04e13c0b);\n",
       "        }\n",
       "            geo_json_4d5e4cf27d48cce90deb9f97bac20397_add({&quot;bbox&quot;: [14.536666666666667, 55.13361111111111, 21.0, 59.562777777777775], &quot;features&quot;: [{&quot;bbox&quot;: [14.536666666666667, 55.13361111111111, 21.0, 59.562777777777775], &quot;geometry&quot;: {&quot;coordinates&quot;: [[[19.07472222222222, 56.62861111111111], [18.50638888888889, 56.34527777777778], [18.01861111111111, 56.09527777777778], [17.55, 55.85], [18.398055555555555, 55.29], [16.23611111111111, 55.13361111111111], [15.063333333333334, 55.94916666666666], [14.83361111111111, 56.10944444444445], [14.536666666666667, 56.31666666666667], [14.889166666666666, 56.501666666666665], [15.117777777777778, 56.628055555555555], [15.242222222222221, 56.683611111111105], [15.66638888888889, 56.86666666666667], [15.875277777777779, 56.9575], [16.828055555555554, 57.371944444444445], [17.946666666666665, 57.94333333333333], [18.043333333333337, 58.17972222222222], [17.960277777777776, 58.66833333333333], [18.136666666666667, 59.01305555555555], [18.47472222222222, 59.117777777777775], [19.983055555555556, 59.562777777777775], [20.54416666666667, 59.25666666666667], [21.0, 59.0], [20.27472222222222, 57.784166666666664], [20.086666666666666, 57.463055555555556], [20.060555555555556, 57.41694444444444], [19.828333333333333, 56.99944444444444], [19.07472222222222, 56.62861111111111]]], &quot;type&quot;: &quot;Polygon&quot;}, &quot;id&quot;: &quot;0&quot;, &quot;properties&quot;: {}, &quot;type&quot;: &quot;Feature&quot;}], &quot;type&quot;: &quot;FeatureCollection&quot;});\n",
       "\n",
       "        \n",
       "    \n",
       "        function geo_json_ff6285bd44774552aeae8aee69fa5ccf_styler(feature) {\n",
       "            switch(feature.id) {\n",
       "                default:\n",
       "                    return {&quot;color&quot;: &quot;black&quot;};\n",
       "            }\n",
       "        }\n",
       "\n",
       "        function geo_json_ff6285bd44774552aeae8aee69fa5ccf_onEachFeature(feature, layer) {\n",
       "            layer.on({\n",
       "            });\n",
       "        };\n",
       "        var geo_json_ff6285bd44774552aeae8aee69fa5ccf = L.geoJson(null, {\n",
       "                onEachFeature: geo_json_ff6285bd44774552aeae8aee69fa5ccf_onEachFeature,\n",
       "            \n",
       "                style: geo_json_ff6285bd44774552aeae8aee69fa5ccf_styler,\n",
       "        });\n",
       "\n",
       "        function geo_json_ff6285bd44774552aeae8aee69fa5ccf_add (data) {\n",
       "            geo_json_ff6285bd44774552aeae8aee69fa5ccf\n",
       "                .addData(data)\n",
       "                .addTo(map_ae6534ca6824cc061b07c61e04e13c0b);\n",
       "        }\n",
       "            geo_json_ff6285bd44774552aeae8aee69fa5ccf_add({&quot;bbox&quot;: [14.3275, 56.9575, 18.136666666666667, 59.28388888888889], &quot;features&quot;: [{&quot;bbox&quot;: [14.3275, 56.9575, 18.136666666666667, 59.28388888888889], &quot;geometry&quot;: {&quot;coordinates&quot;: [[[16.858611111111113, 59.28388888888889], [17.746666666666666, 59.24944444444444], [17.797777777777778, 59.24944444444444], [18.136666666666667, 59.01305555555555], [17.960277777777776, 58.66833333333333], [18.043333333333337, 58.17972222222222], [17.946666666666665, 57.94333333333333], [16.828055555555554, 57.371944444444445], [15.875277777777779, 56.9575], [14.3275, 57.83888888888889], [15.525, 58.47694444444445], [16.20111111111111, 58.85638888888889], [16.467777777777776, 59.032777777777774], [16.661388888888887, 59.159166666666664], [16.858611111111113, 59.28388888888889]]], &quot;type&quot;: &quot;Polygon&quot;}, &quot;id&quot;: &quot;0&quot;, &quot;properties&quot;: {}, &quot;type&quot;: &quot;Feature&quot;}], &quot;type&quot;: &quot;FeatureCollection&quot;});\n",
       "\n",
       "        \n",
       "    \n",
       "                var lat_lng_popup_8ad516e9d6c550fa89601bea30d0ea76 = L.popup();\n",
       "                function latLngPop(e) {\n",
       "                    lat_lng_popup_8ad516e9d6c550fa89601bea30d0ea76\n",
       "                        .setLatLng(e.latlng)\n",
       "                        .setContent(&quot;Latitude: &quot; + e.latlng.lat.toFixed(4) +\n",
       "                                    &quot;&lt;br&gt;Longitude: &quot; + e.latlng.lng.toFixed(4))\n",
       "                        .openOn(map_ae6534ca6824cc061b07c61e04e13c0b);\n",
       "                    }\n",
       "                map_ae6534ca6824cc061b07c61e04e13c0b.on(&#x27;click&#x27;, latLngPop);\n",
       "            \n",
       "&lt;/script&gt;\" style=\"position:absolute;width:100%;height:100%;left:0;top:0;border:none !important;\" allowfullscreen webkitallowfullscreen mozallowfullscreen></iframe></div></div>"
      ],
      "text/plain": [
       "<folium.folium.Map at 0x1edbb164790>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reference_sector = sector_y_esmm\n",
    "m = folium.Map(\n",
    "    [\n",
    "        reference_sector.geometry[0].centroid.y,\n",
    "        reference_sector.geometry[0].centroid.x, \n",
    "    ], \n",
    "    zoom_start=6, tiles='cartodbpositron')\n",
    "folium.GeoJson(\n",
    "    sector_67Y,\n",
    "    style_function=lambda features:{'color': 'blue'}\n",
    ").add_to(m)\n",
    "folium.GeoJson(\n",
    "    sector_w_esmm,\n",
    "    style_function=lambda features:{'color': 'black'}\n",
    ").add_to(m)\n",
    "folium.LatLngPopup().add_to(m)\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a1e754-1181-451b-b70d-fc831fe8a8c7",
   "metadata": {},
   "source": [
    "# Specify the directory of raw files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9532c92b-f36b-48fb-88d5-52b8fc8a36c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folders = [\n",
    "    './raw_data/scat20161015_20161021/',\n",
    "    './raw_data/scat20161112_20161118/',\n",
    "    './raw_data/scat20161210_20161216/',\n",
    "    './raw_data/scat20170107_20170113/',\n",
    "    './raw_data/scat20170215_20170221/',\n",
    "    './raw_data/scat20170304_20170310/',\n",
    "    './raw_data/scat20170401_20170407/',\n",
    "    './raw_data/scat20170429_20170505/',\n",
    "    './raw_data/scat20170527_20170602/',\n",
    "    './raw_data/scat20170624_20170630/',\n",
    "    './raw_data/scat20170722_20170728/',\n",
    "    './raw_data/scat20170819_20170825/',\n",
    "    './raw_data/scat20170916_20170922/'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aee5253-0b5a-438b-bc06-2e8674a77708",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "caaf1dbd-09b2-4580-8b0c-0b1fd6ce9bdc",
   "metadata": {},
   "source": [
    "# Filter relevant flights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88dc92b9-c239-4b31-a995-fe4165c5691d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#####\n",
      "\n",
      "File './processed_data/sector_67Y/intermediate_data/scat20161015_20161021_filtered.geojson' already exists, skipping to next iteration\n",
      "File './processed_data/sector_w_esmm/intermediate_data/scat20161015_20161021_filtered.geojson' already exists, skipping to next iteration\n",
      "All files for week 'scat20161015_20161021' has been generated, moving on to the next week\n",
      "\n",
      "#####\n",
      "\n",
      "File './processed_data/sector_67Y/intermediate_data/scat20161112_20161118_filtered.geojson' already exists, skipping to next iteration\n",
      "File './processed_data/sector_w_esmm/intermediate_data/scat20161112_20161118_filtered.geojson' already exists, skipping to next iteration\n",
      "All files for week 'scat20161112_20161118' has been generated, moving on to the next week\n",
      "\n",
      "#####\n",
      "\n",
      "File './processed_data/sector_67Y/intermediate_data/scat20161210_20161216_filtered.geojson' already exists, skipping to next iteration\n",
      "File './processed_data/sector_w_esmm/intermediate_data/scat20161210_20161216_filtered.geojson' already exists, skipping to next iteration\n",
      "All files for week 'scat20161210_20161216' has been generated, moving on to the next week\n",
      "\n",
      "#####\n",
      "\n",
      "File './processed_data/sector_67Y/intermediate_data/scat20170107_20170113_filtered.geojson' already exists, skipping to next iteration\n",
      "File './processed_data/sector_w_esmm/intermediate_data/scat20170107_20170113_filtered.geojson' already exists, skipping to next iteration\n",
      "All files for week 'scat20170107_20170113' has been generated, moving on to the next week\n",
      "\n",
      "#####\n",
      "\n",
      "File './processed_data/sector_67Y/intermediate_data/scat20170215_20170221_filtered.geojson' already exists, skipping to next iteration\n",
      "File './processed_data/sector_w_esmm/intermediate_data/scat20170215_20170221_filtered.geojson' already exists, skipping to next iteration\n",
      "All files for week 'scat20170215_20170221' has been generated, moving on to the next week\n",
      "\n",
      "#####\n",
      "\n",
      "File './processed_data/sector_67Y/intermediate_data/scat20170304_20170310_filtered.geojson' already exists, skipping to next iteration\n",
      "File './processed_data/sector_w_esmm/intermediate_data/scat20170304_20170310_filtered.geojson' already exists, skipping to next iteration\n",
      "All files for week 'scat20170304_20170310' has been generated, moving on to the next week\n",
      "\n",
      "#####\n",
      "\n",
      "File './processed_data/sector_67Y/intermediate_data/scat20170401_20170407_filtered.geojson' already exists, skipping to next iteration\n",
      "File './processed_data/sector_w_esmm/intermediate_data/scat20170401_20170407_filtered.geojson' already exists, skipping to next iteration\n",
      "All files for week 'scat20170401_20170407' has been generated, moving on to the next week\n",
      "\n",
      "#####\n",
      "\n",
      "File './processed_data/sector_67Y/intermediate_data/scat20170429_20170505_filtered.geojson' already exists, skipping to next iteration\n",
      "File './processed_data/sector_w_esmm/intermediate_data/scat20170429_20170505_filtered.geojson' already exists, skipping to next iteration\n",
      "All files for week 'scat20170429_20170505' has been generated, moving on to the next week\n",
      "\n",
      "#####\n",
      "\n",
      "File './processed_data/sector_67Y/intermediate_data/scat20170527_20170602_filtered.geojson' already exists, skipping to next iteration\n",
      "File './processed_data/sector_w_esmm/intermediate_data/scat20170527_20170602_filtered.geojson' already exists, skipping to next iteration\n",
      "All files for week 'scat20170527_20170602' has been generated, moving on to the next week\n",
      "\n",
      "#####\n",
      "\n",
      "File './processed_data/sector_67Y/intermediate_data/scat20170624_20170630_filtered.geojson' already exists, skipping to next iteration\n",
      "File './processed_data/sector_w_esmm/intermediate_data/scat20170624_20170630_filtered.geojson' already exists, skipping to next iteration\n",
      "All files for week 'scat20170624_20170630' has been generated, moving on to the next week\n",
      "\n",
      "#####\n",
      "\n",
      "File './processed_data/sector_67Y/intermediate_data/scat20170722_20170728_filtered.geojson' already exists, skipping to next iteration\n",
      "File './processed_data/sector_w_esmm/intermediate_data/scat20170722_20170728_filtered.geojson' already exists, skipping to next iteration\n",
      "All files for week 'scat20170722_20170728' has been generated, moving on to the next week\n",
      "\n",
      "#####\n",
      "\n",
      "File './processed_data/sector_67Y/intermediate_data/scat20170819_20170825_filtered.geojson' already exists, skipping to next iteration\n",
      "File './processed_data/sector_w_esmm/intermediate_data/scat20170819_20170825_filtered.geojson' already exists, skipping to next iteration\n",
      "All files for week 'scat20170819_20170825' has been generated, moving on to the next week\n",
      "\n",
      "#####\n",
      "\n",
      "File './processed_data/sector_67Y/intermediate_data/scat20170916_20170922_filtered.geojson' already exists, skipping to next iteration\n",
      "File './processed_data/sector_w_esmm/intermediate_data/scat20170916_20170922_filtered.geojson' already exists, skipping to next iteration\n",
      "All files for week 'scat20170916_20170922' has been generated, moving on to the next week\n",
      "CPU times: total: 31.2 ms\n",
      "Wall time: 6.02 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "processed_data_save_in = \"./processed_data\"\n",
    "if not os.path.exists(processed_data_save_in):\n",
    "    os.mkdir(processed_data_save_in)\n",
    "\n",
    "excluded_files = [\n",
    "    'airspace.json',\n",
    "    'grib_meteo.json'\n",
    "]\n",
    "\n",
    "for each_week_folder in data_folders:\n",
    "    print('\\n#####\\n')\n",
    "    files_to_generate = []\n",
    "    this_week = each_week_folder.split('/')[-2]\n",
    "    for each_sector in sectors_list:\n",
    "        this_sector_folder = f\"{processed_data_save_in}/{each_sector['name']}\"\n",
    "        if not os.path.exists(this_sector_folder):\n",
    "            print(f\"The directory '{this_sector_folder}' does not exist, creating the folder ...\")\n",
    "            os.mkdir(this_sector_folder)\n",
    "        \n",
    "        this_file_name = f\"{this_sector_folder}/intermediate_data/{this_week}_filtered.geojson\"\n",
    "        if os.path.isfile(this_file_name):\n",
    "            print(f\"File '{this_file_name}' already exists, skipping to next iteration\")\n",
    "        else:\n",
    "            print(f\"File '{this_file_name}' does not exists, adding into task list\")\n",
    "            files_to_generate.append([this_file_name, each_sector])\n",
    "    \n",
    "    if len(files_to_generate) == 0:\n",
    "        print(f\"All files for week '{this_week}' has been generated, moving on to the next week\")\n",
    "        continue\n",
    "        \n",
    "    print(f\"Processing week '{this_week}'\")\n",
    "    flight_point_rows = []\n",
    "    for each_raw_file in os.listdir(each_week_folder):\n",
    "        if each_raw_file in excluded_files:\n",
    "            continue\n",
    "        with open(each_week_folder + each_raw_file, 'r') as f:\n",
    "            this_flight = json.load(f)\n",
    "        for each_point in this_flight['plots']:\n",
    "            flight_point_rows.append({\n",
    "                'id': this_flight['id'],\n",
    "                'lat': each_point['I062/105']['lat'],\n",
    "                'lon': each_point['I062/105']['lon']\n",
    "            })\n",
    "    this_df_points = pd.DataFrame(flight_point_rows)\n",
    "    this_gdf_points = gpd.GeoDataFrame(\n",
    "        this_df_points,\n",
    "        geometry=gpd.points_from_xy(\n",
    "            x=this_df_points.lon,\n",
    "            y=this_df_points.lat\n",
    "        ),\n",
    "        crs='EPSG:4326'\n",
    "    )\n",
    "    this_gdf_lines = gpd.GeoDataFrame(\n",
    "        this_gdf_points.groupby(['id'])['geometry'].apply(lambda x: LineString(x.tolist())),\n",
    "        geometry='geometry',\n",
    "        crs=\"EPSG:4326\"\n",
    "    ).reset_index()\n",
    "    \n",
    "    for each_output_task in files_to_generate:\n",
    "        this_sector_gpd = each_output_task[1]['sector_gpd']\n",
    "        this_gdf_lines['in_sector'] = this_gdf_lines.geometry\\\n",
    "            .apply(lambda x: this_sector_gpd.geometry[0].intersects(x))\n",
    "        this_gdf_lines.loc[this_gdf_lines.in_sector==True].to_file(\n",
    "            each_output_task[0],\n",
    "            driver='GeoJSON'\n",
    "        )\n",
    "        print(f\"Generated and saved geojson '{each_output_task[0]}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ffade68-2682-48a7-bf4b-b5ab90952740",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1631e070-39b8-4a1e-b187-a89f939f9883",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3b18091d-4cc0-4c45-a2c4-68daa5e14dd9",
   "metadata": {},
   "source": [
    "# Identify Feasible Flights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d8a02a9-4511-4230-96c8-1e175ddfe4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_format = \"%Y-%m-%dT%H:%M:%S.%f\"\n",
    "d_format_short = \"%Y-%m-%dT%H:%M:%S\"\n",
    "\n",
    "def get_correct_update(fpl_plan_updates, entry_time_str , minutes_ahead=30):\n",
    "    latest = datetime.datetime.strptime(\"1900-01-01T00:00:00.00000\", d_format)\n",
    "    res = False\n",
    "    entry_time = datetime.datetime.strptime(entry_time_str, d_format)\n",
    "    threshold_time = entry_time - datetime.timedelta(0,minutes_ahead*60)\n",
    "    for update in fpl_plan_updates:\n",
    "        time_str = update[\"time_stamp\"]\n",
    "        if \".\" not in time_str:\n",
    "            time_str += \".00\"\n",
    "        timestamp = datetime.datetime.strptime(time_str, d_format)\n",
    "        if timestamp <= threshold_time:\n",
    "            if latest < timestamp:\n",
    "                res = update\n",
    "                latest = timestamp\n",
    "\n",
    "    return res\n",
    "\n",
    "\n",
    "def get_correct_predicted_traj(predicted_traj, entry_time_str , minutes_ahead=30):\n",
    "    latest = datetime.datetime.strptime(\"1900-01-01T00:00:00.00000\", d_format)\n",
    "    # issue: what if the oldest plan is not old enough ?? \n",
    "    res = False\n",
    "    pt_timestamp = None\n",
    "    entry_time = datetime.datetime.strptime(entry_time_str, d_format)\n",
    "    threshold_time = entry_time - datetime.timedelta(0,minutes_ahead*60)\n",
    "    for pt_elem in predicted_traj:\n",
    "        time_str = pt_elem[\"time_stamp\"]\n",
    "        if \".\" not in time_str:\n",
    "            time_str += \".00\"\n",
    "        timestamp = datetime.datetime.strptime(time_str, d_format)\n",
    "        if timestamp <= threshold_time:\n",
    "            if latest < timestamp:\n",
    "                res = pt_elem\n",
    "                latest = timestamp\n",
    "                pt_timestamp = time_str\n",
    "    return res, pt_timestamp\n",
    "        \n",
    "        \n",
    "def get_fp_route(flight_plan_update):\n",
    "    route_str = flight_plan_update['icao_route']\n",
    "    plan_route_strings = flight_plan_update[\"icao_route\"].split(\" \")\n",
    "    route_list = []\n",
    "    for elem in plan_route_strings:\n",
    "        if elem in air_space_dict:\n",
    "            route_list.append(Point(air_space_dict[elem]['lon'], air_space_dict[elem]['lat']))\n",
    "        \n",
    "    if len(route_list) > 1:\n",
    "        return LineString(route_list)\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "def get_route_from_traj(pred_traj):\n",
    "    route_list = []\n",
    "    for point in pred_traj['route']:\n",
    "        route_list.append(Point(point['lon'], point['lat']))\n",
    "    return LineString(route_list)\n",
    "\n",
    "    \n",
    "def get_entry(line, sector):\n",
    "    intersection_line = line.intersection(sector)\n",
    "    if isinstance(intersection_line, MultiLineString):\n",
    "        intersection_line = intersection_line.geoms[0]\n",
    "    intersections = intersection_line.xy\n",
    "    if len(intersections[0]) == 0:\n",
    "        return None, None\n",
    "    entry_lon = intersections[0][0]\n",
    "    entry_lat = intersections[1][0]\n",
    "    return entry_lon, entry_lat\n",
    "\n",
    "def get_position_at_prediction_time(prediction_time: datetime.datetime, input_flight):\n",
    "    prev_point = {}\n",
    "    prev_timestamp = None\n",
    "    for each_point in input_flight['plots']:\n",
    "        try:\n",
    "            point_timestamp = datetime.datetime.strptime(\n",
    "                each_point['time_of_track'], '%Y-%m-%dT%H:%M:%S.%f')\n",
    "        except:\n",
    "            point_timestamp = datetime.datetime.strptime(\n",
    "                each_point['time_of_track'], '%Y-%m-%dT%H:%M:%S')\n",
    "        if point_timestamp > prediction_time:\n",
    "            return prev_point['I062/105']['lon'], prev_point['I062/105']['lat']\n",
    "        prev_point = each_point\n",
    "        prev_timestamp = point_timestamp\n",
    "    return None, None\n",
    "\n",
    "def get_entry_time(A,C,A_time_str,C_time_str,B):\n",
    "    time_A = datetime.datetime.strptime(A_time_str,d_format_short) \n",
    "    time_C = datetime.datetime.strptime(C_time_str,d_format_short)\n",
    "    time_B = time_A + datetime.timedelta(seconds=(time_C-time_A).total_seconds() * (A.distance(B) / A.distance(C)))\n",
    "    return time_B.strftime(d_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06eaa732-9cc8-4734-beab-3e83713774f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "646920ba-bd5c-499e-b4dc-9b11e068b903",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#####\n",
      "\n",
      "Inspecting week 'scat20161015_20161021'\n",
      "File './processed_data/sector_67Y/intermediate_data/scat20161015_20161021_buffer15_results.csv' already exists, skipping to next iteration\n",
      "File './processed_data/sector_w_esmm/intermediate_data/scat20161015_20161021_buffer15_results.csv' already exists, skipping to next iteration\n",
      "\n",
      "#####\n",
      "\n",
      "Inspecting week 'scat20161112_20161118'\n",
      "File './processed_data/sector_67Y/intermediate_data/scat20161112_20161118_buffer15_results.csv' already exists, skipping to next iteration\n",
      "File './processed_data/sector_w_esmm/intermediate_data/scat20161112_20161118_buffer15_results.csv' already exists, skipping to next iteration\n",
      "\n",
      "#####\n",
      "\n",
      "Inspecting week 'scat20161210_20161216'\n",
      "File './processed_data/sector_67Y/intermediate_data/scat20161210_20161216_buffer15_results.csv' already exists, skipping to next iteration\n",
      "File './processed_data/sector_w_esmm/intermediate_data/scat20161210_20161216_buffer15_results.csv' already exists, skipping to next iteration\n",
      "\n",
      "#####\n",
      "\n",
      "Inspecting week 'scat20170107_20170113'\n",
      "File './processed_data/sector_67Y/intermediate_data/scat20170107_20170113_buffer15_results.csv' already exists, skipping to next iteration\n",
      "File './processed_data/sector_w_esmm/intermediate_data/scat20170107_20170113_buffer15_results.csv' already exists, skipping to next iteration\n",
      "\n",
      "#####\n",
      "\n",
      "Inspecting week 'scat20170215_20170221'\n",
      "File './processed_data/sector_67Y/intermediate_data/scat20170215_20170221_buffer15_results.csv' already exists, skipping to next iteration\n",
      "File './processed_data/sector_w_esmm/intermediate_data/scat20170215_20170221_buffer15_results.csv' already exists, skipping to next iteration\n",
      "\n",
      "#####\n",
      "\n",
      "Inspecting week 'scat20170304_20170310'\n",
      "File './processed_data/sector_67Y/intermediate_data/scat20170304_20170310_buffer15_results.csv' already exists, skipping to next iteration\n",
      "File './processed_data/sector_w_esmm/intermediate_data/scat20170304_20170310_buffer15_results.csv' already exists, skipping to next iteration\n",
      "\n",
      "#####\n",
      "\n",
      "Inspecting week 'scat20170401_20170407'\n",
      "File './processed_data/sector_67Y/intermediate_data/scat20170401_20170407_buffer15_results.csv' already exists, skipping to next iteration\n",
      "File './processed_data/sector_w_esmm/intermediate_data/scat20170401_20170407_buffer15_results.csv' already exists, skipping to next iteration\n",
      "\n",
      "#####\n",
      "\n",
      "Inspecting week 'scat20170429_20170505'\n",
      "File './processed_data/sector_67Y/intermediate_data/scat20170429_20170505_buffer15_results.csv' already exists, skipping to next iteration\n",
      "File './processed_data/sector_w_esmm/intermediate_data/scat20170429_20170505_buffer15_results.csv' already exists, skipping to next iteration\n",
      "\n",
      "#####\n",
      "\n",
      "Inspecting week 'scat20170527_20170602'\n",
      "File './processed_data/sector_67Y/intermediate_data/scat20170527_20170602_buffer15_results.csv' already exists, skipping to next iteration\n",
      "File './processed_data/sector_w_esmm/intermediate_data/scat20170527_20170602_buffer15_results.csv' already exists, skipping to next iteration\n",
      "\n",
      "#####\n",
      "\n",
      "Inspecting week 'scat20170624_20170630'\n",
      "File './processed_data/sector_67Y/intermediate_data/scat20170624_20170630_buffer15_results.csv' already exists, skipping to next iteration\n",
      "File './processed_data/sector_w_esmm/intermediate_data/scat20170624_20170630_buffer15_results.csv' already exists, skipping to next iteration\n",
      "\n",
      "#####\n",
      "\n",
      "Inspecting week 'scat20170722_20170728'\n",
      "File './processed_data/sector_67Y/intermediate_data/scat20170722_20170728_buffer15_results.csv' already exists, skipping to next iteration\n",
      "File './processed_data/sector_w_esmm/intermediate_data/scat20170722_20170728_buffer15_results.csv' already exists, skipping to next iteration\n",
      "\n",
      "#####\n",
      "\n",
      "Inspecting week 'scat20170819_20170825'\n",
      "File './processed_data/sector_67Y/intermediate_data/scat20170819_20170825_buffer15_results.csv' already exists, skipping to next iteration\n",
      "File './processed_data/sector_w_esmm/intermediate_data/scat20170819_20170825_buffer15_results.csv' already exists, skipping to next iteration\n",
      "\n",
      "#####\n",
      "\n",
      "Inspecting week 'scat20170916_20170922'\n",
      "File './processed_data/sector_67Y/intermediate_data/scat20170916_20170922_buffer15_results.csv' already exists, skipping to next iteration\n",
      "File './processed_data/sector_w_esmm/intermediate_data/scat20170916_20170922_buffer15_results.csv' already exists, skipping to next iteration\n",
      "CPU times: total: 0 ns\n",
      "Wall time: 4.97 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "buffer_list = [15]\n",
    "\n",
    "for each_week_folder in data_folders:\n",
    "    files_to_generate = []\n",
    "    this_week = each_week_folder.split('/')[-2]\n",
    "    print('\\n#####\\n')\n",
    "    print(f\"Inspecting week '{this_week}'\")\n",
    "    \n",
    "    for each_sector in sectors_list:\n",
    "        this_sector_folder = f\"{processed_data_save_in}/{each_sector['name']}\"     \n",
    "        this_filtered_flights_file = f\"{this_sector_folder}/intermediate_data/{this_week}_filtered.geojson\"\n",
    "        this_filtered_flights = None\n",
    "        sector_geom = each_sector['sector_gpd'].geometry[0]\n",
    "        \n",
    "        for each_buffer in buffer_list:\n",
    "            entries = []\n",
    "            entries_id = []\n",
    "            unavailable_pt = []\n",
    "            plot_start_in_sector = []\n",
    "            not_planned_to_enter = []\n",
    "            file_scanned = 0\n",
    "            \n",
    "            this_filter_stats_file = f\"{this_sector_folder}/intermediate_data/{this_week}_buffer{each_buffer}_stats.json\"\n",
    "            this_filter_result_file = f\"{this_sector_folder}/intermediate_data/{this_week}_buffer{each_buffer}_results.csv\"\n",
    "            if os.path.isfile(this_filter_result_file):\n",
    "                print(f\"File '{this_filter_result_file}' already exists, skipping to next iteration\")\n",
    "                continue\n",
    "            if this_filtered_flights is None:\n",
    "                with open(this_filtered_flights_file, 'r') as f:\n",
    "                    this_filtered_flights = gpd.read_file(f)\n",
    "                    \n",
    "            for each_raw_file in os.listdir(each_week_folder):\n",
    "                if each_raw_file.strip('.json') not in map(str, this_filtered_flights.id.tolist()):\n",
    "                    continue\n",
    "                file_scanned += 1\n",
    "                with open(each_week_folder + each_raw_file, 'r') as f:\n",
    "                    this_flight = json.load(f)\n",
    "                plots = this_flight['plots']\n",
    "                for i, p in enumerate(plots):\n",
    "                    point = Point(p['I062/105']['lon'], p['I062/105']['lat'])\n",
    "                    actual_entry_time = None\n",
    "                    if sector_geom.contains(point):\n",
    "                        # if the plot already starts in the sector, the flight is ignored\n",
    "                        if i == 0:\n",
    "                            plot_start_in_sector.append(each_raw_file.strip('.json'))\n",
    "                            break\n",
    "                        prelim_pred_traj = None # initialize to None so no value is carrid forward\n",
    "                        pred_traj = None # initialize to None so no value is carrid forward\n",
    "                        last_outside_point = Point(plots[i-1]['I062/105']['lon'], plots[i-1]['I062/105']['lat'])\n",
    "                        line = LineString([last_outside_point, point])\n",
    "                        actual_entry_time = p['time_of_track']\n",
    "                        assert actual_entry_time is not None\n",
    "                        if \".\" not in actual_entry_time:\n",
    "                            actual_entry_time += \".00\"\n",
    "                        entry_lon, entry_lat = get_entry(line, sector_geom)\n",
    "                        # this needs to be changed if you want to use flightplan updates\n",
    "                        prelim_pred_traj, prelim_pt_ts = get_correct_predicted_traj(\n",
    "                            this_flight['predicted_trajectory'], \n",
    "                            actual_entry_time, \n",
    "                            each_buffer\n",
    "                        )\n",
    "                        \n",
    "                        if not prelim_pred_traj:\n",
    "                            #print(f\"{file} has no predicted trajectory that is old enough\")\n",
    "                            unavailable_pt.append(each_raw_file.strip('.json'))\n",
    "                            break\n",
    "                        \n",
    "                        prelim_routes = prelim_pred_traj['route']\n",
    "                        forecasted_entry_time = None\n",
    "                        prev_pt_point = None\n",
    "                        pt_point = None\n",
    "                        prev_eto = None\n",
    "                        this_entry_lon = None\n",
    "                        this_entry_lat = None\n",
    "                        for pt_i, pt_p in enumerate(prelim_routes):\n",
    "                            pt_point = Point(pt_p['lon'], pt_p['lat'])\n",
    "                            if prev_pt_point == None:\n",
    "                                prev_pt_point = pt_point\n",
    "                                prev_eto = pt_p['eto']\n",
    "                                continue\n",
    "                            this_pt_line = LineString([prev_pt_point, pt_point])\n",
    "                            if this_pt_line.intersects(sector_geom):\n",
    "                                this_entry_lon, this_entry_lat = get_entry(\n",
    "                                        this_pt_line, sector_geom)\n",
    "                                assert prev_eto is not None\n",
    "                                forecasted_entry_time = get_entry_time(\n",
    "                                        prev_pt_point,\n",
    "                                        pt_point,\n",
    "                                        prev_eto,\n",
    "                                        pt_p['eto'],\n",
    "                                        Point(this_entry_lon, this_entry_lat)\n",
    "                                    )\n",
    "                                break\n",
    "                            prev_pt_point = pt_point\n",
    "                            prev_eto = pt_p['eto']\n",
    "                        #try:\n",
    "                        #    assert forecasted_entry_time is not None\n",
    "                        #except:\n",
    "                        #    print(this_flight['id'])\n",
    "                        #    print(prelim_pt_ts)\n",
    "                        #    print(prev_pt_point, pt_point)\n",
    "                        #finally:\n",
    "                        #    assert forecasted_entry_time is not None\n",
    "                        #assert forecasted_entry_time is not None\n",
    "                        if forecasted_entry_time is not None:\n",
    "                            if \".\" not in forecasted_entry_time:\n",
    "                                forecasted_entry_time += \".00\"\n",
    "                            pred_traj, pt_ts = get_correct_predicted_traj(\n",
    "                                this_flight['predicted_trajectory'],\n",
    "                                forecasted_entry_time,\n",
    "                                each_buffer\n",
    "                            )\n",
    "\n",
    "                            if not pred_traj:\n",
    "                                #print(f\"{file} has no predicted trajectory that is old enough\")\n",
    "                                unavailable_pt.append(each_raw_file.strip('.json'))\n",
    "                                break\n",
    "                            this_prediction_time = datetime.datetime.strptime(\n",
    "                                                        forecasted_entry_time, d_format\n",
    "                                                    ) - datetime.timedelta(minutes=each_buffer)\n",
    "                            this_prediction_time_str = this_prediction_time.strftime(d_format)\n",
    "                        else:\n",
    "                            pred_traj = prelim_pred_traj\n",
    "                            pt_ts = prelim_pt_ts\n",
    "                            this_prediction_time = datetime.datetime.strptime(\n",
    "                                                        actual_entry_time, d_format\n",
    "                                                    ) - datetime.timedelta(minutes=each_buffer)\n",
    "                            this_prediction_time_str = this_prediction_time.strftime(d_format)\n",
    "                            \n",
    "                        fp_entry_lon, fp_entry_lat = get_entry(get_route_from_traj(pred_traj), sector_geom)\n",
    "                        entries_id.append(this_flight['id'])\n",
    "                        actual_lon_at_pred, actual_lat_at_pred = get_position_at_prediction_time(\n",
    "                            this_prediction_time, this_flight)\n",
    "                        this_entry_deviation = None if fp_entry_lon is None else geo_distance.geodesic(\n",
    "                                                    (entry_lat, entry_lon),\n",
    "                                                    (fp_entry_lat, fp_entry_lon)).km\n",
    "                        entries.append({\n",
    "                            'id': this_flight['id'],\n",
    "                            'predicted_trajectory_ts': pt_ts,\n",
    "                            'actual_entry_time': actual_entry_time,\n",
    "                            'forecasted_entry_time': forecasted_entry_time,\n",
    "                            'prediction_time': this_prediction_time_str,\n",
    "                            'plt_entry_lon':entry_lon, \n",
    "                            'plt_entry_lat':entry_lat,\n",
    "                            'fp_entry_lon': fp_entry_lon,\n",
    "                            'fp_entry_lat': fp_entry_lat,\n",
    "                            'at_pred_lon': actual_lon_at_pred,\n",
    "                            'at_pred_lat': actual_lat_at_pred,\n",
    "                            'entry_deviation': this_entry_deviation\n",
    "                        })\n",
    "                        break\n",
    "            # here is each buffer level\n",
    "            this_stats = {\n",
    "                'time_buffer': each_buffer,\n",
    "                'file_scanned': file_scanned,\n",
    "                'unavalable_pt': unavailable_pt,\n",
    "                'plot_start_in_sector': plot_start_in_sector,\n",
    "                'not_planned_to_enter': not_planned_to_enter,\n",
    "                'entries_id': entries_id\n",
    "            }\n",
    "            with open(this_filter_stats_file, 'w') as f_stats:\n",
    "                json.dump(this_stats, f_stats, indent=4)\n",
    "            print(f\"Filter stats file '{this_filter_stats_file}' has been generated and saved\")\n",
    "            df_feasible_flight = pd.DataFrame(entries)\n",
    "            df_feasible_flight.to_csv(this_filter_result_file, index=False)\n",
    "            print(f\"Filter results file '{this_filter_result_file}' has been generated and saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480e378d-2dbc-4416-8cdd-43ced8e152fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "57f8d5c1-6c74-4af9-8fbd-67df5ad9509d",
   "metadata": {},
   "source": [
    "# Combining output files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3a45bb67-32ff-4b24-884f-6bc18f5ea1bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined file './processed_data/sector_67Y/sector_sector_67Y_buffer15_combined_results.csv' has been generated and saved\n",
      "Combined file './processed_data/sector_w_esmm/sector_sector_w_esmm_buffer15_combined_results.csv' has been generated and saved\n",
      "CPU times: total: 797 ms\n",
      "Wall time: 1.07 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "buffer_list = [15]\n",
    "\n",
    "for each_sector in sectors_list:\n",
    "    this_sector_folder = f\"{processed_data_save_in}/{each_sector['name']}\"   \n",
    "    for each_buffer in buffer_list:\n",
    "        this_combined_file_name = f\"{this_sector_folder}/sector_{each_sector['name']}_buffer{each_buffer}_combined_results.csv\"\n",
    "        if os.path.isfile(this_combined_file_name):\n",
    "            print(f\"Combined file '{this_combined_file_name}' already exist, skipping to the next one ...\")\n",
    "            continue\n",
    "        this_df_list = []\n",
    "        for each_week_folder in data_folders:\n",
    "            this_week = each_week_folder.split('/')[-2]\n",
    "            this_sector_folder_input = f\"{processed_data_save_in}/{each_sector['name']}/intermediate_data\"\n",
    "            this_filter_result_file = f\"{this_sector_folder_input}/{this_week}_buffer{each_buffer}_results.csv\"\n",
    "            assert os.path.isfile(this_filter_result_file)\n",
    "            this_df_list.append(pd.read_csv(this_filter_result_file))\n",
    "        this_combined_df = pd.concat(this_df_list, ignore_index=True)\n",
    "        this_combined_df.to_csv(this_combined_file_name, index=False)\n",
    "        print(f\"Combined file '{this_combined_file_name}' has been generated and saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621d2f39-945b-4502-af03-db3acf01625c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0756b06-2c84-4962-af0e-13b52e79c8d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
